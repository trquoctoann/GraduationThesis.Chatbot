{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump\n",
    "from itertools import chain\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  B-Quantity       1.00      1.00      1.00       107\n",
      "     B-Pizza       0.98      0.99      0.99       102\n",
      "     I-Pizza       0.95      0.98      0.97        85\n",
      "   B-Topping       0.99      0.97      0.98       157\n",
      "      B-Size       1.00      0.99      0.99        90\n",
      "      I-Size       0.99      1.00      0.99        86\n",
      "           O       0.99      1.00      0.99       298\n",
      "     B-Crust       0.99      0.96      0.97        73\n",
      "     I-Crust       0.97      0.99      0.98        71\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1069\n",
      "   macro avg       0.98      0.99      0.98      1069\n",
      "weighted avg       0.99      0.99      0.99      1069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PizzaOrderCRFModel:\n",
    "    def __init__(self):\n",
    "        self.model = CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "    \n",
    "    def word2features(self, sentence, i):\n",
    "        word = sentence[i]\n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word[-3:]': word[-3:],\n",
    "            'word[-2:]': word[-2:],\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "        }\n",
    "        if i > 0:\n",
    "            word1 = sentence[i-1]\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:word.isdigit()': word1.isdigit(),\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True\n",
    "\n",
    "        if i < len(sentence)-1:\n",
    "            word1 = sentence[i+1]\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:word.isdigit()': word1.isdigit(),\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True\n",
    "            \n",
    "        return features\n",
    "\n",
    "    def sentence_features(self, words):\n",
    "        return [self.word2features(words, i) for i in range(len(words))]\n",
    "\n",
    "    def sentence_labels(self, labels):\n",
    "        return labels\n",
    "\n",
    "    def load_data_from_json(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return [(\n",
    "            item['words'], \n",
    "            item['label']\n",
    "        ) for item in data]\n",
    "\n",
    "    def train(self, data):\n",
    "        X = [self.sentence_features(s[0]) for s in data]\n",
    "        y = [self.sentence_labels(s[1]) for s in data]\n",
    "\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        X_test = [self.sentence_features(s[0]) for s in data]\n",
    "        y_test = [self.sentence_labels(s[1]) for s in data]\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        y_test_flat = list(chain.from_iterable(y_test))\n",
    "        y_pred_flat = list(chain.from_iterable(y_pred))\n",
    "\n",
    "        labels = ['B-Quantity', 'B-Pizza', 'I-Pizza', 'B-Topping', 'B-Size', 'I-Size', 'O', 'B-Crust', 'I-Crust']\n",
    "        print(classification_report(y_test_flat, y_pred_flat, labels=labels, target_names=labels))\n",
    "\n",
    "    def process_sentence(self, sentence):\n",
    "        tokens = re.findall(r\"[\\w']+|[.,!?;]\", sentence)\n",
    "        return {\n",
    "            \"words\": tokens,\n",
    "            \"label\": [\"O\"] * len(tokens),\n",
    "        }\n",
    "\n",
    "    def predict(self, text):\n",
    "        processed_sentence = self.process_sentence(text)\n",
    "        words = processed_sentence[\"words\"]\n",
    "        features = self.sentence_features(words)\n",
    "\n",
    "        labels = self.model.predict([features])[0]\n",
    "\n",
    "        result = {\n",
    "            \"words\": words,\n",
    "            \"label\": labels,\n",
    "        }\n",
    "        return self.aggregate_entities(result)\n",
    "    \n",
    "    def predict_with_confidence(self, text):\n",
    "        processed_sentence = self.process_sentence(text)\n",
    "        words = processed_sentence[\"words\"]\n",
    "        features = self.sentence_features(words)\n",
    "\n",
    "        labels_with_confidence = self.model.predict_marginals([features])[0]\n",
    "\n",
    "        result = []\n",
    "        for word, word_labels in zip(words, labels_with_confidence):\n",
    "            best_label, best_confidence = max(word_labels.items(), key=lambda item: item[1])\n",
    "            result.append((word, best_label, best_confidence))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def aggregate_entities(self, predicted_result):\n",
    "        aggregated_entities = {}\n",
    "        current_entity = None\n",
    "        current_label = None\n",
    "\n",
    "        for word, label in zip(predicted_result[\"words\"], predicted_result[\"label\"]):\n",
    "            if label.startswith(\"B-\"):\n",
    "                if current_entity is not None and current_label is not None:\n",
    "                    if current_label in aggregated_entities:\n",
    "                        aggregated_entities[current_label].append(\" \".join(current_entity))\n",
    "                    else:\n",
    "                        aggregated_entities[current_label] = [\" \".join(current_entity)]\n",
    "                \n",
    "                current_entity = [word]\n",
    "                current_label = label[2:] \n",
    "            elif label.startswith(\"I-\") and current_entity is not None and label[2:] == current_label:\n",
    "                current_entity.append(word)\n",
    "            else:\n",
    "                if current_entity is not None and current_label is not None:\n",
    "                    if current_label in aggregated_entities:\n",
    "                        aggregated_entities[current_label].append(\" \".join(current_entity))\n",
    "                    else:\n",
    "                        aggregated_entities[current_label] = [\" \".join(current_entity)]\n",
    "                    current_entity = None\n",
    "                    current_label = None\n",
    "                if label == \"O\":\n",
    "                    continue\n",
    "                else:\n",
    "                    aggregated_entities[label] = aggregated_entities.get(label, []) + [word]\n",
    "\n",
    "        if current_entity is not None and current_label is not None:\n",
    "            if current_label in aggregated_entities:\n",
    "                aggregated_entities[current_label].append(\" \".join(current_entity))\n",
    "            else:\n",
    "                aggregated_entities[current_label] = [\" \".join(current_entity)]\n",
    "\n",
    "        return aggregated_entities\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        dump(self.model, file_path)\n",
    "\n",
    "model = PizzaOrderCRFModel()\n",
    "data = model.load_data_from_json('../data/labeled/entity/order/data_20240331.json')\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "model.train(train_data)\n",
    "model.evaluate(test_data)\n",
    "model.save_model(\"homie.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tokenize_dictionary(dictionary_path=\"utils/tokenize_dictionary.json\"):\n",
    "    with open(dictionary_path, 'r', encoding=\"utf-8\") as file:\n",
    "        tokenize_dictionary = json.load(file)\n",
    "    return tokenize_dictionary\n",
    "\n",
    "def read_stop_word_dictionary(dictionary_path=\"utils/vietnamese-stopwords.txt\"):\n",
    "    with open(dictionary_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        stopwords_dictionary = file.read()\n",
    "    return set(stopwords_dictionary.split(\"\\n\"))\n",
    "\n",
    "def lowercase_text(text: str): \n",
    "    return text.lower()\n",
    "\n",
    "def remove_diacritic(text: str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)]).replace(\"đ\", \"d\")\n",
    "\n",
    "def tokenize(text: str, tokenize_dictionary: dict):\n",
    "    sorted_items = sorted(tokenize_dictionary.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    for original, token in sorted_items:\n",
    "        pattern = re.compile(r'\\b' + re.escape(original) + r'\\b', re.IGNORECASE)\n",
    "        text = pattern.sub(token, text)\n",
    "    return text\n",
    "\n",
    "def combined_tokenize(text: str, tokenize_dictionary: dict):\n",
    "    tokenized_text = tokenize(text, tokenize_dictionary)\n",
    "\n",
    "    list_token_tokenized_text = tokenized_text.split()\n",
    "    token_diacritic_map = {}\n",
    "    no_diacritic_text = \"\"\n",
    "    for index, token in enumerate(list_token_tokenized_text):\n",
    "        if \"_\" not in token:\n",
    "            token_diacritic_map[remove_diacritic(token)] = index\n",
    "            no_diacritic_text += remove_diacritic(token) + \" \"\n",
    "    no_diacritic_text = no_diacritic_text.strip()\n",
    "\n",
    "    no_diacritic_tokenized_text = tokenize(no_diacritic_text, tokenize_dictionary)\n",
    "    for no_diacritic_token in no_diacritic_tokenized_text.split():\n",
    "        if \"_\" in no_diacritic_token:\n",
    "            start_of_word = float('inf')\n",
    "            end_of_word = float('-inf')\n",
    "            for part_token in no_diacritic_token.split(\"_\"):\n",
    "                part_token = remove_diacritic(part_token)\n",
    "                start_of_word = min(start_of_word, token_diacritic_map.get(part_token))\n",
    "                end_of_word = max(end_of_word, token_diacritic_map.get(part_token)) + 1\n",
    "            list_token_tokenized_text[start_of_word : end_of_word] = [no_diacritic_token]\n",
    "    \n",
    "    return \" \".join([token for token in list_token_tokenized_text])\n",
    "\n",
    "def remove_stopwords(text: str, stopwords_dictionary: set):\n",
    "    stopwords_regex = '|'.join(re.escape(stopword) for stopword in sorted(stopwords_dictionary, key=len, reverse=True))\n",
    "    text = re.sub(r'\\b(?:' + stopwords_regex + r')(?:\\W|$)', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def ner_preprocessing(text: str, tokenize_dictionary: dict = read_tokenize_dictionary(), stopwords_dictionary: set = read_stop_word_dictionary()):\n",
    "    text = lowercase_text(text)\n",
    "    text = combined_tokenize(text, tokenize_dictionary)\n",
    "    text = remove_stopwords(text, stopwords_dictionary)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', 'B-Quantity', 0.9370998094622134),\n",
       " ('cái_pizza', 'B-Pizza', 0.8716720002742258),\n",
       " ('pepperoni', 'I-Pizza', 0.7060395886143229),\n",
       " (',', 'O', 0.9989631037075085),\n",
       " ('một', 'B-Quantity', 0.9985210497087567),\n",
       " ('cái', 'O', 0.9820262970032756),\n",
       " ('l', 'B-Size', 0.4532072267317428),\n",
       " (',', 'O', 0.9951493906272788),\n",
       " ('thịt_hun_khói', 'B-Topping', 0.9474141000952374),\n",
       " (',', 'O', 0.9959466172695189),\n",
       " ('nấm_rơm', 'B-Topping', 0.9941752417474755),\n",
       " ('ớt_xanh', 'B-Topping', 0.9947987666980466),\n",
       " ('một', 'B-Quantity', 0.9935378491490322),\n",
       " ('cái', 'O', 0.9959443061005397),\n",
       " ('size', 'B-Size', 0.9785916245302014),\n",
       " ('xl', 'I-Size', 0.9969574829506358),\n",
       " ('ba_chỉ_bò_nướng', 'B-Topping', 0.7799210896855255)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"đặt 2 cái pizza pepperoni, một cái L, thịt hun khói, nấm rơm ớt xanh và một cái size XL thêm bả chì bố nướng\"\n",
    "model.predict_with_confidence(ner_preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Quantity': ['2', 'một', 'một'],\n",
       " 'Pizza': ['cái_pizza pepperoni'],\n",
       " 'Size': ['size l', 'size m'],\n",
       " 'Topping': ['thịt_hun_khói']}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(ner_preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
