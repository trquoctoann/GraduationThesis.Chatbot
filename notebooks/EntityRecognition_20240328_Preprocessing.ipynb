{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import py_vncorenlp\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = os.getcwd().replace(\"\\\\\", \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tokenize_dictionary(dictionary_path=\"utils/tokenize_dictionary.json\"):\n",
    "    with open(dictionary_path, 'r', encoding=\"utf-8\") as file:\n",
    "        tokenize_dictionary = json.load(file)\n",
    "    return tokenize_dictionary\n",
    "\n",
    "def read_stop_word_dictionary(dictionary_path=\"utils/vietnamese-stopwords.txt\"):\n",
    "    with open(dictionary_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        stopwords_dictionary = file.read()\n",
    "    return set(stopwords_dictionary.split(\"\\n\"))\n",
    "\n",
    "def lowercase_text(text: str): \n",
    "    return text.lower()\n",
    "\n",
    "def remove_diacritic(text: str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)]).replace(\"Ä‘\", \"d\")\n",
    "\n",
    "# def tokenize(text: str, tokenize_dictionary: dict):\n",
    "#     tokenized_sentence = text\n",
    "#     for original, token in tokenize_dictionary.items():\n",
    "#         pattern = re.compile(re.escape(original), re.IGNORECASE)\n",
    "#         tokenized_sentence = pattern.sub(token, tokenized_sentence)\n",
    "#     return tokenized_sentence\n",
    "\n",
    "def tokenize(text: str, tokenize_dictionary: dict):\n",
    "    sorted_items = sorted(tokenize_dictionary.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    for original, token in sorted_items:\n",
    "        pattern = re.compile(r'\\b' + re.escape(original) + r'\\b', re.IGNORECASE)\n",
    "        text = pattern.sub(token, text)\n",
    "    return text\n",
    "\n",
    "def combined_tokenize(text: str, tokenize_dictionary: dict):\n",
    "    tokenized_original = tokenize(text, tokenize_dictionary)\n",
    "\n",
    "    replace_map = {}\n",
    "    for key in tokenize_dictionary.keys():\n",
    "        no_accents_key = remove_diacritic(key)\n",
    "        if no_accents_key != key:\n",
    "            replace_map[no_accents_key.lower()] = tokenize_dictionary[key]\n",
    "    final_text = ' '.join([replace_map.get(remove_diacritic(word).lower(), word) for word in tokenized_original.split()])\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "# def remove_stopwords(text: str, stopwords_dictionary: set):\n",
    "#     for stopword in stopwords_dictionary:\n",
    "#         text = re.sub(r'\\b' + re.escape(stopword) + r'\\b', '', text)\n",
    "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#     return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def remove_stopwords(text: str, stopwords_dictionary: set):\n",
    "    stopwords_regex = '|'.join(re.escape(stopword) for stopword in sorted(stopwords_dictionary, key=len, reverse=True))\n",
    "    text = re.sub(r'\\b(?:' + stopwords_regex + r')(?:\\W|$)', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def ner_preprocessing(text: str, tokenize_dictionary: dict, stopwords_dictionary: set):\n",
    "    text = lowercase_text(text)\n",
    "    text = combined_tokenize(text, tokenize_dictionary)\n",
    "    text = remove_stopwords(text, stopwords_dictionary)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_dictionary = read_tokenize_dictionary()\n",
    "stopwords_dictionary = read_stop_word_dictionary()\n",
    "with open(\"../data/raw/entity/order/processed_ideal_order.txt\", 'r', encoding='utf-8') as input_file:\n",
    "    with open(\"../data/processed/entity/order/processed_ideal_order.txt\", 'w', encoding='utf-8') as output_file:\n",
    "        for line in input_file:\n",
    "            processed_line = ner_preprocessing(line.strip(), tokenize_dictionary, stopwords_dictionary)\n",
    "            output_file.write(processed_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
